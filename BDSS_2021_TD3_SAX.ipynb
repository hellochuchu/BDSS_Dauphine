{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDSS_2021_TD3_SAX.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "M-9rovwlHP_X",
        "V8i6vqq1FuQh",
        "hge1MvnJx-r0",
        "l_U6dtDNyaDX",
        "lCtZp-qE8ooM",
        "Z9IRkMvO98C_",
        "5nhqhzsCKC37",
        "GCgCGY89Vf81",
        "xt-Q_w01qgkO"
      ],
      "authorship_tag": "ABX9TyN8hP6B8gViq14Spw59l94h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasgneccoh/BDSS_Dauphine/blob/main/BDSS_2021_TD3_SAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bases de données semi-structurées - TD 3\n",
        "\n",
        "Welcome to the TD 3. This part will cover SAX\n"
      ],
      "metadata": {
        "id": "M-9rovwlHP_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preambule"
      ],
      "metadata": {
        "id": "V8i6vqq1FuQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "import re\n",
        "from xml.dom.minidom import parse\n",
        "import xml.sax\n",
        "\n",
        "# Functions to work with XML files\n",
        "\n",
        "def validate_xml(xml_path:str, dtd_path:str) -> bool:\n",
        "    ''' Validate an XML file  against a DTD using the lxml library\n",
        "    '''\n",
        "    try:\n",
        "        dtd = etree.DTD(open(dtd_path))\n",
        "    except etree.DTDParseError as ed:\n",
        "        print(f\"DTDParseError: {ed}\")\n",
        "        for i, er in enumerate(ed.error_log):\n",
        "            print(f\"\\t{i}-> {er.message}, at line {er.line}\")\n",
        "        etree.clear_error_log()\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        xml_doc = etree.parse(xml_path)\n",
        "    except etree.XMLSyntaxError as e:\n",
        "        print(f\"XMLSyntaxError: {e}\")\n",
        "        for i, er in enumerate(e.error_log):\n",
        "            print(f\"\\t{i}-> {er.message}, at line {er.line}\")\n",
        "        etree.clear_error_log()\n",
        "        return False\n",
        "\n",
        "    result = dtd.validate(xml_doc)\n",
        "    if not result: print(dtd.error_log[0])\n",
        "\n",
        "    return result\n",
        "\n",
        "def write_xml_dtd_files_from_strings(xml_strings, dtd_strings, identifiers = None):\n",
        "    ''' Write a list of strings into files. This strings should be XML and DTD files\n",
        "    '''\n",
        "\n",
        "    # If single strings are given, encapsulate them in lists  \n",
        "    if all(map(lambda o: isinstance(o, str), [xml_strings, dtd_strings])):\n",
        "        xml_strings, dtd_strings = [xml_strings], [dtd_strings]\n",
        "\n",
        "    if len(xml_strings) != len(dtd_strings):\n",
        "        raise Exception(\"Different number of XML and DTD strings!\")\n",
        "\n",
        "    # If no identifiers are given, create default ones. This determines file names\n",
        "    if identifiers is None:\n",
        "        identifiers = [f\"file_{i}\" for i in range(len(xml_strings))]\n",
        "\n",
        "    try:\n",
        "        for x, d, id in zip(xml_strings, dtd_strings, identifiers):\n",
        "            xml_path, dtd_path = f\"{id}.xml\", f\"{id}.dtd\" \n",
        "            with open(xml_path,\"w\") as f:\n",
        "                f.write(x)\n",
        "            with open(dtd_path,\"w\") as f:\n",
        "                f.write(d)\n",
        "    except Exception as e:\n",
        "        print(\"Problems while writing XML and DTD files\")\n",
        "        raise e\n",
        "\n",
        "    return identifiers\n",
        "\n",
        "\n",
        "\n",
        "def test_validation(xml_string, dtd_string, validator):\n",
        "    ''' Validate an XML document against a DTD, both given as strings\n",
        "    '''\n",
        "    # Write files\n",
        "    write_xml_dtd_files_from_strings(xml_string, dtd_string, identifiers = ['temp'])\n",
        "    \n",
        "    # Validate\n",
        "    return validator(\"temp.xml\", \"temp.dtd\" )\n",
        "\n",
        "def xpath_query_xml_string(xml_string, query_string):\n",
        "    xml_path = \"xml_doc.xml\"\n",
        "    with open(xml_path, \"w\") as f:\n",
        "        # Remove all whitespaces to keep the 'real' text of each node\n",
        "        f.write(re.sub(\">[\\s|\\n]*<\", \"><\", xml_string))\n",
        "        f.close()\n",
        "    xml_doc = etree.parse(xml_path)\n",
        "    query = etree.XPath(query_string)\n",
        "    return query(xml_doc)\n",
        "\n",
        "def xpath_query_xml_file(xml_path, query_string):\n",
        "    xml_doc = etree.parse(xml_path)\n",
        "    query = etree.XPath(query_string)\n",
        "    return query(xml_doc)\n",
        "\n",
        "\n",
        "def print_xpath_query_results(results):\n",
        "    print(f\"Total results: {len(results)}\")\n",
        "    print(\"*\"*20 + \"\\n\")\n",
        "    for e in results:\n",
        "        try:        \n",
        "            print(f\"node tag: {e.tag}\")\n",
        "            print(f\"node text: *{e.text}*\")\n",
        "            print(', '.join([f\"{k} = {v}\"for k, v in e.items()]))\n",
        "            print(\"-\"*20)\n",
        "        except:\n",
        "            print(\"--Except\")\n",
        "            print(e)\n"
      ],
      "metadata": {
        "id": "SwLF3qxcFmqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAX\n",
        "Download the two datasets that we have been using\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "[Big dataset](https://universitedauphine-my.sharepoint.com/:u:/g/personal/lucas_gnecco-heredia_universitedauphine_onmicrosoft_com/EeHHLB6bFw9CsjLovqeVwFUBA6ORQ8IOoFfZq9fsjYjNAA)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "[Small dataset](https://universitedauphine-my.sharepoint.com/:u:/g/personal/lucas_gnecco-heredia_universitedauphine_onmicrosoft_com/EaAWThKBY9JDtIgh5QrkfvYBWGJfAuZvymOvZUKHinXKFA)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "hge1MvnJx-r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Print only certain elements"
      ],
      "metadata": {
        "id": "l_U6dtDNyaDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrinterContentHandler(xml.sax.ContentHandler):\n",
        "    def __init__(self, tags_to_print = None):\n",
        "        super().__init__()\n",
        "        self.tags_to_print = tags_to_print\n",
        "        \n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        if not self.tags_to_print is None and name in self.tags_to_print:\n",
        "            print(\"startElement: '\" + name + \"'\")\n",
        "        return\n",
        "\n",
        "    def endElement(self, name):\n",
        "        print(\"endElement: '\" + name + \"'\\n\")\n",
        "        return\n",
        "\n",
        "    def characters(self, content):\n",
        "        # When text is encountered\n",
        "        print(\"Characters: \" + content + \"\")\n",
        "        return\n",
        "\n",
        "\n",
        "tags_to_print = [\"FILM\"]\n",
        "handler = PrinterContentHandler(tags_to_print = tags_to_print)\n",
        "\n",
        "path = \"films.xml\"\n",
        "f = open(path)\n",
        "\n",
        "xml.sax.parse(f, handler)"
      ],
      "metadata": {
        "id": "jIOY0to7yY4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Get the titles"
      ],
      "metadata": {
        "id": "lCtZp-qE8ooM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetTextInsideTag(xml.sax.ContentHandler):\n",
        "    def __init__(self, tag = None):\n",
        "        super().__init__()\n",
        "        self.tag = tag\n",
        "        self.reading = False\n",
        "        self.buffer = []\n",
        "        self.result = []\n",
        "        \n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        if not self.tag is None and name == self.tag:\n",
        "            self.reading = True\n",
        "        return\n",
        "\n",
        "\n",
        "    def endElement(self, name):\n",
        "        if self.buffer:\n",
        "            # If something was read, then add it to results\n",
        "            # before reseting the buffer\n",
        "            self.result.append(' '.join(self.buffer))\n",
        "        self.reading = False\n",
        "        self.buffer = []\n",
        "        return\n",
        "\n",
        "    def characters(self, content):\n",
        "        # When text is encountered\n",
        "        # print(\"Characters '\" + content + \"'\")\n",
        "        if self.reading: self.buffer.append(content)\n",
        "        return\n",
        "\n",
        "\n",
        "tag = \"TITRE\"\n",
        "handler = GetTextInsideTag(tag = tag)\n",
        "\n",
        "path = \"imdb_sample.xml\"\n",
        "f = open(path)\n",
        "\n",
        "xml.sax.parse(f, handler)\n",
        "\n",
        "print(handler.result)"
      ],
      "metadata": {
        "id": "PS4s356Q8mnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now in dom\n",
        "memoryBefore = getMemoryInfo()[3]/1e6\n",
        "print(f\"Memory usage before parsing document with DOM: {memoryBefore}\")\n",
        "dom = parse(\"imdb_sample.xml\")\n",
        "memoryAfter = getMemoryInfo()[3]/1e6\n",
        "print(f\"Memory usage after parsing document with DOM: {memoryAfter}\")\n",
        "\n",
        "print(f\"Difference: {memoryAfter - memoryBefore}\")\n",
        "\n",
        "del dom\n",
        "\n",
        "memoryAfter = getMemoryInfo()[3]/1e6\n",
        "print(f\"Memory usage after parsing document with DOM and delete: {memoryAfter}\")\n",
        "\n",
        "# I write this function to simplify getting the value of a node that only contains text\n",
        "def getText(node):\n",
        "    return node.childNodes[0].data\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Query 1\n",
        "# La liste des titres de films.      \n",
        "\n",
        "def dom_query_1_1(dom):\n",
        "    titre=[]\n",
        "    for t in dom.getElementsByTagName(\"TITRE\"):\n",
        "        titre.append(t.childNodes[0].data)\n",
        "    return titre\n",
        "\n",
        "def dom_query_1_2(dom):\n",
        "    titles = []\n",
        "    for f in dom.getElementsByTagName(\"FILM\"):\n",
        "        for t in f.getElementsByTagName(\"TITRE\"):\n",
        "            titles.append(getText(t))\n",
        "    return titles"
      ],
      "metadata": {
        "id": "xAEfhxplF6ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Get the titles of films staring some artist"
      ],
      "metadata": {
        "id": "Z9IRkMvO98C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetFilmsByArtistInCast(xml.sax.ContentHandler):\n",
        "    def __init__(self, prenom, nom):\n",
        "        super().__init__()\n",
        "        self.prenomSearch = prenom\n",
        "        self.nomSearch = nom\n",
        "\n",
        "        self.result = []\n",
        "        self.titleBuffer = []\n",
        "        self.prenomBuffer = []\n",
        "        self.nomBuffer = []\n",
        "\n",
        "        self.titleTemp = None\n",
        "        self.prenomTemp = None\n",
        "        self.nomTemp = None\n",
        "        \n",
        "        self.whereInDoc = None\n",
        "\n",
        "        self.reading = False\n",
        "        \n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        self.whereInDoc = name\n",
        "        if name in [\"PRENOM\", \"NOM\", \"TITRE\"]:\n",
        "            self.reading = True\n",
        "        return\n",
        "\n",
        "    def endElement(self, name):\n",
        "        # If we read something that we need, then get the contentn and use it\n",
        "        if name == \"TITRE\":\n",
        "            self.titleTemp = ' '.join(self.titleBuffer)\n",
        "            self.titleBuffer = []\n",
        "        if name == \"PRENOM\":\n",
        "            self.prenomTemp = ' '.join(self.prenomBuffer)\n",
        "            self.prenomBuffer = []\n",
        "        if name == \"NOM\":\n",
        "            self.nomTemp = ' '.join(self.nomBuffer)\n",
        "            self.nomBuffer = []\n",
        "\n",
        "        # If we end reading a ROLE element, we can check if it contains the \n",
        "        # artist we want\n",
        "        if name == \"ROLE\":\n",
        "            if self.prenomSearch == self.prenomTemp and self.nomSearch == self.nomTemp:\n",
        "                self.result.append(self.titleTemp)\n",
        "        self.reading = False\n",
        "        return\n",
        "\n",
        "    def characters(self, content):\n",
        "        if self.reading:\n",
        "        # If we are in TITLE, we have to save it\n",
        "            if self.whereInDoc == \"TITRE\": self.titleBuffer.append(content)\n",
        "            if self.whereInDoc == \"PRENOM\": self.prenomBuffer.append(content)\n",
        "            if self.whereInDoc == \"NOM\": self.nomBuffer.append(content)\n",
        "        return\n",
        "\n",
        "    def endDocument(self):\n",
        "        self.titleBuffer = []\n",
        "        self.prenomBuffer = []\n",
        "        self.nomBuffer = []\n",
        "\n",
        "\n",
        "prenom, nom = \"Bruce\", \"Willis\"\n",
        "handler = GetFilmsByArtistInCast(prenom = prenom, nom = nom)\n",
        "\n",
        "path = \"imdb_sample.xml\"\n",
        "f = open(path)\n",
        "\n",
        "xml.sax.parse(f, handler)\n",
        "\n",
        "print(handler.result)"
      ],
      "metadata": {
        "id": "Nciaru1u-Lwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 1: Queries with SAX on the movie dataset\n",
        "\n",
        "Try to do some other queries using SAX.\n",
        "Compare your results (and maybe even running times!) with other tools like XPath or DOM.\n",
        "\n",
        "I suggest the queries 1 to 8, and then query 11"
      ],
      "metadata": {
        "id": "5eCeJkwzCk9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6nNRy_rTKAeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data science with text\n",
        "\n",
        "Here I want to introduce you to some more advanced topics in Data Science and Machine Learning.\n",
        "\n",
        "We will use it as an excuse to practice SAX\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Bag of words and TF-IDF**\n",
        "\n",
        "In Data Science and Machine Learning, particularly in Natural Language Processing, the objects to study are text documents. There are different ways to study them, but from a mathematical perspective we need ways of encoding such text documents into more \"vectorial\" data\n",
        "\n",
        "The question becomes: ***How do you transform a piece of text into a vector to apply your algorithms on them?***\n",
        "\n",
        "\n",
        "One very common example is sentiment analysis. The basic idea is that you want to know if some text (for example a movie review or a tweet) is positive or negative towards a subject. This can be seen as a classification problem.\n",
        "\n",
        "\n",
        "The initial approach to turn a piece of text into a vector is the Bag of Words, where you characterize a document by the words that appear in it and their frequence. A more sophisticated approach can be TF-IDF that takes into account the number of words in each document and also the relative frequence of words across documents.\n",
        "\n",
        "See this site for a detailed and simple exmplanation if you have doubts\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
        "\n",
        "\n",
        "In this excercise we will try to create such document vectors for our movie dataset using SAX as a way to read the data."
      ],
      "metadata": {
        "id": "5nhqhzsCKC37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words\n",
        "\n",
        "We need to go through the document, get the RESUME and build a vocabulary that contains all the words present in all the RESUMEs.\n",
        "\n",
        "We do not care about every word. We will remove unecessary words using a special library\n",
        "\n",
        "We are going to optimize our code and create the bag of words as we go through the resumes"
      ],
      "metadata": {
        "id": "GCgCGY89Vf81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 2.1.1 \n",
        "Write a function that given some text, it eliminates all non important words\n",
        "and returns a list of words representing the text\n",
        "We will use a library called spacy for the stop words in french and we can use\n",
        "another library called gensim to help us do some other preprocessing\n",
        "\n",
        "'''\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import gensim.parsing.preprocessing as prep\n",
        "import re\n",
        "\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), lambda x: re.sub('\\W+',' ',x) ,\\\n",
        "                    prep.strip_tags, prep.strip_punctuation, \\\n",
        "                    prep.strip_punctuation2, prep.strip_multiple_whitespaces, \\\n",
        "                    prep.strip_numeric, \\\n",
        "                    prep.strip_short]\n",
        "our_prep_func = lambda x: prep.preprocess_string(x, CUSTOM_FILTERS)\n",
        "\n",
        "\n",
        "# Traiter le texte\n",
        "resume = \"Pulp Fiction décrit l'odyssée sanglante et burlesque de petits malfrats \\\n",
        "dans la jungle de Hollywood, ou s'entrecroisent les destins de deux petits \\\n",
        "tueurs, d'un dangereux gangster marié à une camée, d'un boxeur roublard, de \\\n",
        "prêteurs sur gages sadiques, d'un caïd élégant et dévoué, d'un dealer bon \\\n",
        "mari et de deux tourtereaux à la gachette facile...\"\n",
        "\n",
        "# Lets test the two approaches to see the difference\n",
        "new = [s for s in resume.split() if s not in fr_stop]\n",
        "print(new)\n",
        "\n",
        "new = [s for s in our_prep_func(resume) if s not in fr_stop]\n",
        "print(new)\n",
        "\n",
        "\n",
        "# This is the actual function\n",
        "def process_text(text, processing, stopwords = fr_stop):\n",
        "    return [s for s in processing(text) if s not in stopwords]\n",
        "\n",
        "\n",
        "print(\"Our final function\")\n",
        "new = process_text(resume, our_prep_func, fr_stop)\n",
        "print(new)"
      ],
      "metadata": {
        "id": "l3O23avnS6Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 2.1.2\n",
        "Defining the word count of a document\n",
        "Write a function that given a list of words and a vocabulary, computes the word\n",
        "count representation of the text\n",
        "The vocabulary will be represented as a dictionary containing pairs (word, index)\n",
        "where index is the position of the word in the vocabulary\n",
        "\n",
        "We will suppose that all the words are in the vocabulary\n",
        "'''\n",
        "\n",
        "def bag_of_words(tokens, vocab):\n",
        "    ''' Your code here '''\n",
        "    return \n",
        "\n",
        "\n",
        "# Test it with a simple example\n",
        "tokens = [\"test\", \"sentence\", \"test\", \"test\", \"horse\", \"sentence\"]\n",
        "vocab = {\"sentence\":0, \"horse\":1, \"test\":2}\n",
        "bow = bag_of_words(tokens, vocab)\n",
        "print(bow)\n",
        "\n"
      ],
      "metadata": {
        "id": "EgJolpfpcrIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 2.1.3\n",
        "Expanding a vocabulary\n",
        "When we read new text, some new words might appear. We need to add them to the\n",
        "vocabulary we are considering.\n",
        "Write a function that given a vocabulary and some new text (already preprocessed),\n",
        "adds the new words to the vocabulary (if there are new words)\n",
        "'''\n",
        "\n",
        "def expand_vocab(vocab, new_tokens):\n",
        "    ''' Your code here '''\n",
        "    return \n",
        "\n",
        "\n",
        "# Test\n",
        "tokens = [\"test\", \"new_word\", \"horse\", \"more_novelty\"]\n",
        "vocab = {\"sentence\":0, \"horse\":1, \"test\":2}\n",
        "expand_vocab(vocab, tokens)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "Ip9A8zfwfkVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 2.1.4\n",
        "Put it all together\n",
        "Now use the functions we created and SAX to build the bag of words representation\n",
        "for all the resumes\n",
        "\n",
        "Note: For the tests, use this dummy dataset so that we can see something\n",
        "    https://universitedauphine-my.sharepoint.com/:u:/g/personal/lucas_gnecco-heredia_universitedauphine_onmicrosoft_com/EV8zxLKQpo5NgV4KUoOllWUBXgBWAOuZoq-OjIBTEsc_YQ?e=dMk2td\n",
        "\n",
        "Note: We will take care of the length of the vectors later\n",
        "'''\n",
        "\n",
        "class ResumeBagOfWordsSax(xml.sax.ContentHandler):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vectors = {}\n",
        "        self.vocab = {}\n",
        "        \n",
        "    def startElement(self, name, attrs):\n",
        "        return \n",
        "\n",
        "    def endElement(self, name):\n",
        "        return\n",
        "\n",
        "    def characters(self, content):\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "handler = ResumeBagOfWordsSax()\n",
        "\n",
        "# Remember to download the file and upload it to the Colab session\n",
        "path = \"imdb_simple_example.xml\"\n",
        "f = open(path)\n",
        "\n",
        "xml.sax.parse(f, handler)\n",
        "\n",
        "print(*handler.vectors.items(), sep=\"\\n\")\n"
      ],
      "metadata": {
        "id": "jOHrXZgZkICh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Lets fill the vectors with zeros to get them to the right length\n",
        "'''\n",
        "N = len(handler.vocab)\n",
        "for k, v in handler.vectors.items():\n",
        "    v += [0]*(N-len(v))\n",
        "\n",
        "print(\"New vectors after resize\")\n",
        "print(*handler.vectors.items(), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "yktmPf0duqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "We can create a table to better understand each vector\n",
        "\n",
        "NOTE: This only makes sense with dummy datasets\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.DataFrame(data = handler.vectors, index = handler.vocab.keys())\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "kW-1IScsun3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "Now that we have the Bag of Words for each resume, we can create the TF-IDF representations\n",
        "\n",
        "This can be done in serveral ways, and you can try the one you think is more fun\n",
        "\n",
        "\n",
        "\n",
        "*   Use plain Python\n",
        "*   Use *numpy* (vectors, vector operations)\n",
        "*   Use *pandas* (easiest way IMO)\n",
        "\n"
      ],
      "metadata": {
        "id": "xt-Q_w01qgkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rReIEPiTqf8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}